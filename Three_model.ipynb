{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#combination of three models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_pickle('train_df.pkl')\n",
    "test_df = pd.read_pickle('test_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\pandas\\core\\generic.py:5096: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[name] = value\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "test_df['primary_use'] = le.fit_transform(test_df['primary_use']).astype(np.int8)\n",
    "\n",
    "#change meter_reading value for site_0 train data according to Sohier Dane from Kaggle Team\n",
    "#https://www.kaggle.com/c/ashrae-energy-prediction/discussion/119261\n",
    "train_df[train_df[\"site_id\"]==0].meter_reading = train_df[train_df[\"site_id\"]==0].meter_reading * 0.2931\n",
    "\n",
    "test_df[\"month\"] = test_df[\"timestamp\"].apply(lambda x: int(x[5:7]))\n",
    "test_df[\"hour\"] = test_df[\"timestamp\"].apply(lambda x: int(x[11:13]))\n",
    "\n",
    "#handling missing values\n",
    "train_df['floor_count'] = train_df['floor_count'].fillna(-999).astype(np.int16)\n",
    "test_df['floor_count'] = test_df['floor_count'].fillna(-999).astype(np.int16)\n",
    "\n",
    "train_df['year_built'] = train_df['year_built'].fillna(-999).astype(np.int16)\n",
    "test_df['year_built'] = test_df['year_built'].fillna(-999).astype(np.int16)\n",
    "\n",
    "train_df['age'] = train_df['age'].fillna(-999).astype(np.int16)\n",
    "test_df['age'] = test_df['age'].fillna(-999).astype(np.int16)\n",
    "\n",
    "train_df['cloud_coverage'] = train_df['cloud_coverage'].fillna(-999).astype(np.int16)\n",
    "test_df['cloud_coverage'] = test_df['cloud_coverage'].fillna(-999).astype(np.int16) \n",
    "\n",
    "del train_df[\"timestamp\"], test_df[\"timestamp\"]\n",
    "categoricals = [\"site_id\", \"building_id\", \"primary_use\",  \"meter\",  \"month\", \"hour\", \"day_of_week\"]\n",
    "drop_cols = [\"sea_level_pressure\", \"wind_speed\",\"wind_direction\"]\n",
    "\n",
    "numericals = [\"square_feet\", \"year_built\", \"air_temperature\", \"cloud_coverage\",\n",
    "              \"dew_temperature\", 'precip_depth_1_hr', 'floor_count', 'beaufort_scale']\n",
    "\n",
    "feat_cols = categoricals + numericals\n",
    "target = np.log1p(train_df[\"meter_reading\"])\n",
    "\n",
    "del train_df[\"meter_reading\"] \n",
    "\n",
    "train_df = train_df.drop(drop_cols, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-fold\n",
    "folds = 5\n",
    "seed = 2019\n",
    "shuffle = False\n",
    "kf = KFold(n_splits=folds, shuffle=shuffle, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_X = train_df[feat_cols].iloc[train_index]\n",
    "#val_X = train_df[feat_cols].iloc[val_index]\n",
    "#train_y = target.iloc[train_index]\n",
    "#val_y = target.iloc[val_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lgbm\n",
    "import lightgbm as lgb\n",
    "\n",
    "def lgb_model(train_x,train_y,val_x,val_y,test,verbose):\n",
    "    params = {\n",
    "            'boosting_type': 'gbdt',\n",
    "            'objective': 'regression',\n",
    "            'metric': {'rmse'},\n",
    "            'subsample_freq': 1,\n",
    "            'learning_rate': 0.3,\n",
    "            'bagging_freq': 5,\n",
    "            'num_leaves': 330,\n",
    "            'feature_fraction': 0.9,\n",
    "            'lambda_l1': 1,  \n",
    "            'lambda_l2': 1\n",
    "            }\n",
    "    lgb_train = lgb.Dataset(train_X, train_y, categorical_feature=categoricals)\n",
    "    lgb_eval = lgb.Dataset(val_X, val_y, categorical_feature=categoricals)\n",
    "    model = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=500,\n",
    "                valid_sets=(lgb_train, lgb_eval),\n",
    "                early_stopping_rounds=50,\n",
    "                verbose_eval = 50)\n",
    "    best_idx = np.argmin(np.array(record['valid_0']['rmse']))\n",
    "\n",
    "    val_pred = model.predict(val_x, num_iteration = model.best_iteration)\n",
    "    test_pred = model.predict(test, num_iteration = model.best_iteration)\n",
    "    \n",
    "    return {'val':val_pred, 'test':test_pred, 'error':record['valid_0']['rmse'][best_idx], 'importance':model.feature_importance('gain')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'libxgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-42a9abb92a3d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#XGBoosting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mlibxgboost\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mval_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mval_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'libxgboost'"
     ]
    }
   ],
   "source": [
    "#XGBoosting\n",
    "import libxgboost as xgb\n",
    "\n",
    "def xgb_model(train_x,train_y,val_x,val_y,test,verbose):\n",
    "    \n",
    "    params = {'objective': 'reg:linear', \n",
    "              'eta': 0.01, \n",
    "              'max_depth': 6, \n",
    "              'subsample': 0.6, \n",
    "              'colsample_bytree': 0.7,  \n",
    "              'eval_metric': 'rmse', \n",
    "              'seed': random_seed, \n",
    "              'silent': True,\n",
    "    }\n",
    "    \n",
    "    record = dict()\n",
    "    model = xgb.train(params\n",
    "                      , xgb.DMatrix(train_x, train_y)\n",
    "                      , 100000\n",
    "                      , [(xgb.DMatrix(train_x, train_y), 'train'), (xgb.DMatrix(val_x, val_y), 'valid')]\n",
    "                      , verbose_eval=verbose\n",
    "                      , early_stopping_rounds=500\n",
    "                      , callbacks = [xgb.callback.record_evaluation(record)])\n",
    "    best_idx = np.argmin(np.array(record['valid']['rmse']))\n",
    "\n",
    "    val_pred = model.predict(xgb.DMatrix(val_x), ntree_limit=model.best_ntree_limit)\n",
    "    test_pred = model.predict(xgb.DMatrix(test), ntree_limit=model.best_ntree_limit)\n",
    "\n",
    "    return {'val':val_pred, 'test':test_pred, 'error':record['valid']['rmse'][best_idx], 'importance':[i for k, i in model.get_score().items()]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "#from sklearn.ensemble import forest\n",
    "#def ran_for(train_x,train_y,val_x,val_y,test):\n",
    "    \n",
    "#model = RandomForestRegressor(n_estimators=60,\n",
    "#                             random_state=0,n_jobs=-1)\n",
    "#   model.fit(train_x,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = dict()\n",
    "val_pred = np.zeros(train_df.shape[0])\n",
    "test_pred = np.zeros(test_df.shape[0])\n",
    "final_err = 0\n",
    "verbose = False\n",
    "\n",
    "for i, (trn, val) in enumerate(kf) :\n",
    "    print(i+1, \"fold.    RMSE\")\n",
    "    \n",
    "    trn_x = train_df.loc[trn, :]\n",
    "    trn_y = y[trn]\n",
    "    val_x = train_df.loc[val, :]\n",
    "    val_y = y[val]\n",
    "    \n",
    "    fold_val_pred = []\n",
    "    fold_test_pred = []\n",
    "    fold_err = []\n",
    "    \n",
    "    #\"\"\" xgboost\n",
    "    start = datetime.now()\n",
    "    result = xgb_model(trn_x, trn_y, val_x, val_y, test, verbose)\n",
    "    fold_val_pred.append(result['val']*0.2)\n",
    "    fold_test_pred.append(result['test']*0.2)\n",
    "    fold_err.append(result['error'])\n",
    "    print(\"xgb model.\", \"{0:.5f}\".format(result['error']), '(' + str(int((datetime.now()-start).seconds/60)) + 'm)')\n",
    "    #\"\"\"\n",
    "    \n",
    "    #\"\"\" lightgbm\n",
    "    start = datetime.now()\n",
    "    result = lgb_model(trn_x, trn_y, val_x, val_y, test, verbose)\n",
    "    fold_val_pred.append(result['val']*0.4)\n",
    "    fold_test_pred.append(result['test']*0.4)\n",
    "    fold_err.append(result['error'])\n",
    "    print(\"lgb model.\", \"{0:.5f}\".format(result['error']), '(' + str(int((datetime.now()-start).seconds/60)) + 'm)')\n",
    "    #\"\"\"\n",
    "    \n",
    "    # mix result of multiple models\n",
    "    val_pred[val] += np.mean(np.array(fold_val_pred), axis = 0)\n",
    "    #print(fold_test_pred)\n",
    "    #print(fold_test_pred.shape)\n",
    "    #print(fold_test_pred.columns)\n",
    "    test_pred += np.mean(np.array(fold_test_pred), axis = 0) / k\n",
    "    final_err += (sum(fold_err) / len(fold_err)) / k\n",
    "    \n",
    "    print(\"---------------------------\")\n",
    "    print(\"avg   err.\", \"{0:.5f}\".format(sum(fold_err) / len(fold_err)))\n",
    "    print(\"blend err.\", \"{0:.5f}\".format(np.sqrt(np.mean((np.mean(np.array(fold_val_pred), axis = 0) - val_y)**2))))\n",
    "    \n",
    "    print('')\n",
    "    \n",
    "print(\"fianl avg   err.\", final_err)\n",
    "print(\"fianl blend err.\", np.sqrt(np.mean((val_pred - y)**2)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
